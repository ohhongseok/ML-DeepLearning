{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "확률적 경사 하강법.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCdjYXxuOFVkl7hBeGDIDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohhongseok/ML-DeepLearning/blob/main/4/4-2/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC_%ED%95%98%EA%B0%95%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이번장 키워드\n",
        " - 확률적 경사하강법\n",
        " - 손실함수\n",
        " - 에포크\n",
        " - 대량의 데이터에서 분류 모델을 훈련하는 방법을 배우기\n",
        "\n",
        "# 점진적 학습\n",
        " - IDEA : 훈련데이터가 조금씩 쌓여간다(한번에 데이터를 쌓는게 아닌, 계속 조금씩 측정해 나감)\n",
        " - 이럴때 데이터에 대한 훈련 및 저장은 어떻게? \n",
        "  > SOL1. 기존 훈련데이터에 새로운 데이터를 계속 추가해 모델을 그때그때 다시 학습 <br>\n",
        "   - 초반에는 괜찮지만, 몇달 몇년이 지난 후 모델의 학습 시간 및 데이터를 저장하기위한 추가적인 자원이 필요 따라서 <br>지속가능한 방법은 아님<br>\n",
        "\n",
        "  >SOL2. 새로운 데이터를 추가할 때 이전 데이터를 버림으로써 훈련데이터의 크기를 일정하게 유지하는 것\n",
        "   - 데이터셋의 크기는 비교적 일정하게 유지되겠지만, 데이터를 버릴 때 중요한 데이터가 버려질 수 있다.\n",
        "\n",
        "  >***SOL3. 이전에 훈련한 모델을 버리지 않고, 새로운 데이터에 대해서 조금씩만 훈련하는 방식*** \n",
        "   - 이런 방식이면, 기존 데이터를 포함하여 새로 학습하지 않아도 되며, 앞서 학습한 모델에 대해서도 잊지 않을 것\n",
        "   - 이런 방식의 학습방법을 ***점진적 학습(온라인 학습)*** 이라고 한다. \n",
        "   - 점진적 학습의 대표적인 방법으로는 ***확률적 경사 하강법***이 있다.\n",
        "\n",
        "# 확룰적 경사하강법\n",
        " - 무작위하게 고른 경사를 따라 내려가는 방법\n",
        " - 일반적인 등산에서 생각해 보면, 경사가 가장 빠른 곳이 가장 빨리 내려옴, 경사하강법도 마찬가지로 가장 가파른 경사를 따라 원하는 <br>지점으로 내려오는것이 목표 \n",
        " - 마찬가지로 일반적인 등산에서 생각해 보면, 안전을 위해 조금씩 천천히 내려와야 한다. 경사하강법도 마찬가지로, 경사가 가장 가파른 곳을 찾지만, 조금씩 내려오는 것이 중요하다.\n",
        " - 가장 가파른 길을 찾는 방법으로는 전체 샘플을 모두 사용하지 않고, 딱 하나의 샘플을 훈련세트에서 랜덤하게 골라 가장 가파른 길을 찾는다.\n",
        " - 랜덤하게 하나의 샘플을 사용하여 경사를 조금 내려가고, 그다음 훈련 세트에 랜덤하게 또다른 샘플을 하나 선택하여 경사를 조금 내려간다.<br>\n",
        " 이러한 방법으로 전체 샘플을 모두 사용할 때 까지 계속 진행한다.\n",
        " - 만약 이러한 방법으로 전체의 샘플을 사용했음에도 산을 다 못 내려왔으면, 다시 처음부터 시작하며, 모든 샘플을 다시 채워 넣는다.\n",
        " - 이렇게 ***훈련세트를 한번 모두 사용***하는 과정을 ***에포크(epoch)*** 라고 하며, 경사하강법은 수십, 수백번의 에포크를 사용한다.\n",
        " - 확률적 경사하강법과 다르게, 한 학습 turn 당 여러개의 샘플데이터를 뽑아 경사를 내려가는 방식은 ***미니배치 경사 하강법*** 이라 한다.\n",
        " - 극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용할 수 도 있다.\n",
        " 이를 ***배치 경사 하강법*** 이라 한다.\n",
        "  > 배치 경사 하강법은 전체 데이터를 사용하기에 가장 안정적인 방법이지만, 전체 데이터를 사용하기에 컴퓨터의 리소스를 <br>많이 사용하게 된다.\n",
        " - 확률적 경사하깅법 : 훈련 세트를 사용해 산 아래에 있는 최적의 장소로 조금씩 가까워 지게 이동하는 알고리즘, 따라서 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트 되어도 계속 이어나갈 수 있다.\n",
        "\n",
        "# 확률적 경사하강법이 내려가려는 산\n",
        " - 확률적 경사하강법을 통해 내려가려는 산을 ***손실함수(비용함수)*** 라고 한다.\n",
        " - 손실함수는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지 측정하는 기준임. 따라서 손실함수의 값이 작을수록 좋은 값이지만, <br>어떤값이 최솟값인지는 모름 \n",
        " - 가능한 많이 기울기를 찾아 내려가 봐야 함(따라서 천천히 많은 기울기를 보는게 중요 - 확률적 경사 하강법)\n",
        " - 최종적으로 결정을 내릴땐, 어떠한 값이 최솟값인지 알아야 함.\n",
        "\n",
        "# 손실함수\n",
        "\n",
        "# 내일 noteshelf -> 인공지능 기말 -> 신경망 2 -> 오차함수 찾아보기"
      ],
      "metadata": {
        "id": "Cx0fODSm1It8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYQpfNyZ0IHG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}